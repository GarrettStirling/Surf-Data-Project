{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "#Directory\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project')\n",
    "\n",
    "#File path\n",
    "filepath = 'C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project\\\\data_raw\\\\Surf_Data_2020_12_30_v2.xlsx'\n",
    "\n",
    "#Reading files\n",
    "    #Surf Data Sheet:\n",
    "df_2020 = pd.read_excel(filepath, sheet_name='2020')\n",
    "df_2019 = pd.read_excel(filepath, sheet_name='2019')\n",
    "df_2018 = pd.read_excel(filepath, sheet_name='2018')\n",
    "df_2017 = pd.read_excel(filepath, sheet_name='2017')\n",
    "df_coordinates = pd.read_excel(filepath, sheet_name='Coordinates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting them all together (Only Date, Spot, and Region are the same)\n",
    "df_all = pd.concat([df_2017, df_2018, df_2019, df_2020], ignore_index=True)\n",
    "#remove some columns in all b/c not needed in the entire dataset\n",
    "df_all = df_all.drop(columns =\n",
    "                     ['People', 'Texture', 'Wave Quality', 'Wave Height',\n",
    "                      'Visuals', 'Notes', 'Surfing Quality', 'Sky',\n",
    "                      'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', \n",
    "                      'Unnamed: 20','BUOY Data', 'Hrs', 'Board', 'Wetty'])\n",
    "#Make the date columns date\n",
    "df_all['Date'] = pd.to_datetime(df_all[['Year', 'Month', 'Day']])\n",
    "df_all['DateTime'] = df_all['Date'].dt.strftime('%Y/%m/%d %H:%M') #This is used for the kepler.gl animation\n",
    "\n",
    "#Compile just the 2018-2020 data (Used for wave height)\n",
    "savethesecolumns = ['Year', 'Month', 'Day', 'Wave Height', 'Wave Quality']\n",
    "df_181920 = pd.concat([df_2018[savethesecolumns], \n",
    "                       df_2019[savethesecolumns], \n",
    "                       df_2020[savethesecolumns]], \n",
    "                      ignore_index=True)\n",
    "df_181920['Date'] = pd.to_datetime(df_181920[['Year', 'Month', 'Day']])\n",
    "\n",
    "#Change the wave height seperated by a comma into a single value (WH_avg)\n",
    "df_181920 = pd.concat([df_181920[['Date', 'Year', 'Month', 'Day']], df_181920['Wave Height'].str.split(',', expand=True)], axis=1) #Split into two columns\n",
    "df_181920['wh1'] = pd.to_numeric(df_181920.iloc[:, 4]) #Change object to int\n",
    "df_181920['wh2'] = pd.to_numeric(df_181920.iloc[:, 5]) #Change object to int\n",
    "df_181920['WH_avg'] = df_181920[['wh1', 'wh2']].mean(axis=1) #get the mean and assign it\n",
    "df_181920 = df_181920.drop(columns = ['wh1', 'wh2', 0, 1]) #remove some columns\n",
    "   \n",
    "#Adding to the 2018/2019/2020 concat df so we can do an analysis of waves at a spot\n",
    "df_181920['wv_ql'] = pd.concat([df_2018['Wave Quality'], \n",
    "                                df_2019['Wave Quality'], \n",
    "                                df_2020['Wave Quality']], ignore_index=True)\n",
    "df_181920['Spot'] = pd.concat([df_2018['Spot'], \n",
    "                               df_2019['Spot'], \n",
    "                               df_2020['Spot']], ignore_index=True)\n",
    "df_181920['Region'] = pd.concat([df_2018['Region'], \n",
    "                                 df_2019['Region'], \n",
    "                                 df_2020['Region']], ignore_index=True)\n",
    "    \n",
    "#Changing Directory before saving\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project\\\\data_exported')\n",
    "\n",
    "#Save the data from 2018-2020\n",
    "df_181920.to_csv('df_181920.csv', index=False, header=True)\n",
    "\n",
    "#Changing Directory back\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSIS OF SPOT QUALITY/CONSISTENCY\n",
    "\n",
    "df_spotavg = pd.DataFrame()\n",
    "df_spotavg['wv_ql_avg'] = round(df_181920.groupby('Spot')['wv_ql'].mean(), 2) #Find mean of the spot and round the value\n",
    "df_spotavg['spot_count'] = df_181920.groupby('Spot')['wv_ql'].count() # Add the counts\n",
    "\n",
    "#Grouping by spot and intereating through each spot to find the amount of times wave_qual is above 9\n",
    "above9 = []\n",
    "for (spotname, group) in df_181920.groupby('Spot'):\n",
    "    groupinlist = group.wv_ql.tolist() \n",
    "    counter = 0\n",
    "    for j in groupinlist: #\n",
    "        if j > 8.9:\n",
    "            counter = counter + 1\n",
    "    above9.append(counter)\n",
    "df_spotavg['abv9_count'] = above9\n",
    "df_spotavg['abv9_prop'] = round(df_spotavg['abv9_count'] / df_spotavg['spot_count'], 2)\n",
    "\n",
    "\n",
    "df_spotavg = df_spotavg.drop(df_spotavg[df_spotavg['spot_count'] < 10].index) #Removes colums if the spot was only surfed x amount of times\n",
    "# df_spotavg = df_spotavg.sort_values(by='wv_ql_avg', ascending = False) #sorts by the wave quality\n",
    "# df_spotavg = df_spotavg.sort_values(by='abv9_count', ascending = False) #sorts by the wave quality\n",
    "df_spotavg = df_spotavg.sort_values(by='abv9_prop', ascending = False) #sorts by the wave quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSIS OF REGION QUALITY/CONSISTENCY\n",
    "\n",
    "df_regionavg = pd.DataFrame()\n",
    "#Find mean of the region and round the value\n",
    "df_regionavg['wv_ql_avg'] = round(df_181920.groupby('Region')['wv_ql'].mean(), 2) \n",
    "df_regionavg['region_count'] = df_181920.groupby('Region')['wv_ql'].count() # Add the counts\n",
    "\n",
    "#Grouping by Region and intereating through each Region to find the amount of times wave_qual is above 9\n",
    "regionabove9 = []\n",
    "for (regionname, group) in df_181920.groupby('Region'):\n",
    "    groupinlist = group.wv_ql.tolist() \n",
    "    counter = 0\n",
    "    for j in groupinlist: #\n",
    "        if j > 8.9:\n",
    "            counter = counter + 1\n",
    "    regionabove9.append(counter)\n",
    "df_regionavg['abv9_count'] = regionabove9\n",
    "df_regionavg['abv9_prop'] = round(df_regionavg['abv9_count'] / df_regionavg['region_count'], 2)\n",
    "\n",
    "\n",
    "df_regionavg = df_regionavg.drop(df_regionavg[df_regionavg['region_count'] < 10].index) #Removes colums if the spot was only surfed x amount of times\n",
    "# df_spotavg = df_spotavg.sort_values(by='wv_ql_avg', ascending = False) #sorts by the wave quality\n",
    "# df_spotavg = df_spotavg.sort_values(by='abv9_count', ascending = False) #sorts by the wave quality\n",
    "df_regionavg = df_regionavg.sort_values(by='abv9_prop', ascending = False) #sorts by the wave quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hrs_sum</th>\n",
       "      <th>count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.0</td>\n",
       "      <td>14</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154.5</td>\n",
       "      <td>21</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.5</td>\n",
       "      <td>9</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>108.5</td>\n",
       "      <td>16</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>148.0</td>\n",
       "      <td>25</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>117.0</td>\n",
       "      <td>21</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109.5</td>\n",
       "      <td>17</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>82.5</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>173.0</td>\n",
       "      <td>27</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>217.5</td>\n",
       "      <td>30</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>137.0</td>\n",
       "      <td>21</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>86.0</td>\n",
       "      <td>15</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>155.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>132.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>79.0</td>\n",
       "      <td>14</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>90.5</td>\n",
       "      <td>16</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>147.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>150.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>148.5</td>\n",
       "      <td>26</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>247.0</td>\n",
       "      <td>38</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hrs_sum  count  year  month\n",
       "0     198.0     28  2019      1\n",
       "1      84.0     14  2019      2\n",
       "2     154.5     21  2019      3\n",
       "3     100.0     16  2019      4\n",
       "4      51.5      9  2019      5\n",
       "5     108.5     16  2019      6\n",
       "6     148.0     25  2019      7\n",
       "7     117.0     21  2019      8\n",
       "8      43.0      7  2019      9\n",
       "9     109.5     17  2019     10\n",
       "10     82.5     11  2019     11\n",
       "11    173.0     27  2019     12\n",
       "12    217.5     30  2020      1\n",
       "13    137.0     21  2020      2\n",
       "14     86.0     15  2020      3\n",
       "15     20.0      5  2020      4\n",
       "16    155.0     28  2020      5\n",
       "17    132.0     29  2020      6\n",
       "18     79.0     14  2020      7\n",
       "19     90.5     16  2020      8\n",
       "20    147.0     28  2020      9\n",
       "21    150.0     29  2020     10\n",
       "22    148.5     26  2020     11\n",
       "23    247.0     38  2020     12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AMOUNT OF TIMES/HRS IN WATER\n",
    "\n",
    "#Adding 2019/2020 concat df so we can do an analysis of waves at a spot\n",
    "df_1920 = pd.DataFrame()\n",
    "df_1920['Month'] = pd.concat([df_2019['Month'], df_2020['Month']], ignore_index=True)\n",
    "df_1920['Year'] = pd.concat([df_2019['Year'], df_2020['Year']], ignore_index=True)\n",
    "df_1920['Hrs'] = pd.concat([df_2019['Wave Quality'], df_2020['Wave Quality']], ignore_index=True)\n",
    "\n",
    "#This functions input the year and outputs the monthly average bins for that year\n",
    "def month_sum(year):\n",
    "    df = pd.DataFrame()\n",
    "    df['hrs'] = df_1920.loc[df_1920['Year'] == year, 'Hrs']\n",
    "    df['month'] = df_1920.loc[df_1920['Year'] == year, 'Month']\n",
    "    df_output = pd.DataFrame()\n",
    "    df_output['hrs_sum'] = df.groupby('month')['hrs'].sum()   \n",
    "    df_output['count'] = df.groupby('month')['hrs'].count()\n",
    "    df_output['year'] = year\n",
    "    df_output['month'] = np.arange(1, 13)\n",
    "    return df_output\n",
    "\n",
    "#calling the function above and creating a new dataframe with the month, year, and month avg\n",
    "df_monthly_time = pd.concat([month_sum(2019), month_sum(2020)], ignore_index=True)\n",
    "df_monthly_time = round(df_monthly_time, 2)\n",
    "\n",
    "df_monthly_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the coordinates df with the spots df to put lat long on every spot sequentialy\n",
    "df_test = pd.merge(df_all[['Spot']], df_coordinates, on='Spot', how='left')\n",
    "\n",
    "#Get Data Ready For geospatial Animation\n",
    "df_animation = pd.DataFrame()\n",
    "df_animation['DateTime'] = df_all['DateTime']\n",
    "df_animation['Spot'] = df_all['Spot']\n",
    "df_animation['Latitude'] = df_test['Lat']\n",
    "df_animation['Longitude'] = df_test['Long']\n",
    "df_animation['Value'] = 10\n",
    "\n",
    "#Changing Directory before saving\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project\\\\data_exported')\n",
    "\n",
    "#Saving the data\n",
    "df_animation.to_csv('animation_ready.csv', index=False)\n",
    "\n",
    "#Changing directory after saving\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find unique spots within the coordinates sheet that do no corresond to the spots in the 'years' sheets\n",
    "unique_coor_spots = np.unique(df_coordinates['Spot'])\n",
    "unique_all_spots = np.unique(df_all['Spot'])\n",
    "\n",
    "#Find spots which I surfed that I do not have coordinate attached to them\n",
    "need_coor = []\n",
    "for i in range(0, len(unique_all_spots)):\n",
    "    if unique_all_spots[i] not in unique_coor_spots:\n",
    "        print(unique_all_spots[i])\n",
    "        need_coor.append(unique_all_spots[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Occurance of spots!\n",
    "df_all_occur = df_all['Spot'].value_counts()\n",
    "\n",
    "#Turn occurence into a dataframe\n",
    "df_all_occur = pd.DataFrame(df_all_occur)\n",
    "#Reset the index and change the names of the columns\n",
    "df_all_occur = df_all_occur.reset_index()\n",
    "df_all_occur.rename(columns = {'index': 'Spot', 'Spot': 'Occurrence'}, inplace = True) \n",
    "\n",
    "#Now merge the coordinates with occurance\n",
    "df_all_coor_occur = pd.merge(df_coordinates, df_all_occur, how='inner', on = 'Spot')\n",
    "df_all_coor_occur = df_all_coor_occur.drop(columns=['Unnamed: 3'])\n",
    "df_all_coor_occur.rename(columns = {'Lat': 'Latitude', 'Long': 'Longitude'}, inplace = True) \n",
    "\n",
    "#Changing Directory befroe saving\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project\\\\data_exported')\n",
    "\n",
    "#Saving the data\n",
    "df_all_coor_occur.to_csv('Coor_with_Occur.csv', index=False)\n",
    "\n",
    "#Changing directory after saving\n",
    "os.chdir('C:\\\\Users\\\\Garrett\\\\Google Drive\\\\Documents\\\\Journal\\\\Surf Data Project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
